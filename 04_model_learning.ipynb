{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7226, 17)\n",
      "(803, 17)\n",
      "(7226, 7)\n",
      "(803, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = np.load('./news_data_max_17_size_9656.npy', allow_pickle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 17, 300)           2896800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 17, 32)            48032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 17, 32)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 17, 128)           62208     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 17, 64)            37248     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 17, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 64)                24960     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 3,078,471\n",
      "Trainable params: 3,078,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "# Embedding: 다차원 공간 상에 word를 배치해, 의미벡터 부여. 비슷한 위치 파악.\n",
    "'''\n",
    "같은 자리에 들어가서 말이 된다면 유사하다고 볼 수 있음\n",
    "오늘 날씨는 축구/야구/데이트하기 좋다 (!)\n",
    "오늘 날씨는 딸기/엄마/커피하기 좋다 (?)\n",
    "유사한 단어는 공간 상에 비슷한 위치에 둘 수 있다\n",
    "'''\n",
    "# Conv1D: 문장 1D\n",
    "# 벡터사이즈 큰 것으로 고를 것임\n",
    "\n",
    "'''\n",
    "\"t\" 다음에 올 수 있는 철자는? \"t\" 다음에 \"e\"가 온다면?\n",
    "이처럼 이전의 예측 결과는 다음 예측 결과에 영향을 준다 = RNN\n",
    "RNN의 문제: 반복이 많다. 따라서 미분을 하다보면 기울기 소실 문제가 발생한다.\n",
    "레이어가 깊어질수록, input_length가 길어질수록 값이 작아져서\n",
    "(0.5보다 작은 값에 계속 곱하기 때문에) 학습 결과가 좋지 않다.\n",
    "\n",
    "RNN의 단점을 보완한 것이 LSTM: 레이어를 건너 뛰어 모델에 넘겨주는 것.\n",
    "따라서 순서정보가 중요할 때는 주로 LSTM을 쓴다.\n",
    "\n",
    "이때 GRU는 LSTM과 비슷하게 순서정보를 학습하는 모델\n",
    "tanh함수: LSTM이나 GRU에 쓴다. 이건 정해진 것이다.\n",
    "'''\n",
    "# return_sequences: 30개 문장이 입력되면, 30개 문장이 출력되도록 (False하면 맨 마지막 거 하나만 나옴)\n",
    "# GRU한 결과를 또 GRU에 넣을 거면 True로 해야 한다\n",
    "\n",
    "# Dropout: 매우 촘촘하게 연결된 Dense레이어에서 랜덤하게 일부를 학습하지 않는다.(망각)\n",
    "# 과적합을 방지하기 위함이다.\n",
    "\n",
    "# 성능 개선을 위해 몇 가지 더 빼고 추가함(수정 여러번 해서 확인함)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(9656, 300, input_length=17)) # 300차원\n",
    "model.add(Conv1D(filters=32, kernel_size = 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "\n",
    "# 3층의 GRU\n",
    "model.add(GRU(128, activation = 'tanh', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(GRU(64, activation = 'tanh', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(GRU(64, activation = 'tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense 레이어; 다중분류이므로 softmax\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7226 samples, validate on 803 samples\n",
      "Epoch 1/6\n",
      "7226/7226 [==============================] - 16s 2ms/sample - loss: 1.4858 - accuracy: 0.3896 - val_loss: 0.8632 - val_accuracy: 0.6849\n",
      "Epoch 2/6\n",
      "7226/7226 [==============================] - 7s 1ms/sample - loss: 0.6179 - accuracy: 0.7822 - val_loss: 0.5793 - val_accuracy: 0.8082\n",
      "Epoch 3/6\n",
      "7226/7226 [==============================] - 7s 1ms/sample - loss: 0.2541 - accuracy: 0.9231 - val_loss: 0.6762 - val_accuracy: 0.7970\n",
      "Epoch 4/6\n",
      "7226/7226 [==============================] - 7s 980us/sample - loss: 0.1411 - accuracy: 0.9572 - val_loss: 0.7280 - val_accuracy: 0.8107\n",
      "Epoch 5/6\n",
      "7226/7226 [==============================] - 7s 976us/sample - loss: 0.0897 - accuracy: 0.9751 - val_loss: 0.8879 - val_accuracy: 0.8057\n",
      "Epoch 6/6\n",
      "7226/7226 [==============================] - 7s 944us/sample - loss: 0.0620 - accuracy: 0.9839 - val_loss: 0.8785 - val_accuracy: 0.8107\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "# validation시 학습하지 않도록\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "fit_hist = model.fit(X_train, Y_train, batch_size=100, epochs = 6, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./news_finance.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_model = load_model('./news_finance.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
